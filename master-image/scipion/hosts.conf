[localhost]
PARALLEL_COMMAND = mpirun -np %_(JOB_NODES)d %_(COMMAND)s
NAME = KUBERNETES
MANDATORY = 1
SUBMIT_COMMAND = bash %_(JOB_SCRIPT)s
SUBMIT_TEMPLATE = #!/bin/bash
    echo $$
    echo "time:-, ntasks: %_(JOB_NODES)d, cpu-per-task: %_(JOB_THREADS)d, mem: %_(JOB_MEMORY)s, gpu:-, reguires_gpu:-"
    echo '%_(JOB_COMMAND)s' >> /home/scipionuser/Desktop/job-cmd.txt

    echo "/home/scipionuser/scipion3/scipion3 run %_(JOB_COMMAND)s"
    #/home/scipionuser/scipion3/scipion3 run %_(JOB_COMMAND)s >> /home/scipionuser/Desktop/job-cmd-out.log 2>&1 &

    #envsubst < /opt/k8s/depl-tool.yaml | if [ "%_(JOB_REQUIRES_GPU)s" == "True" ]; then sed '/cerit.io\/gpu.mem/s/^#//'; else cat; fi | kubectl apply -f - | cut -d' ' -f1 > /home/scipionuser/Desktop/jobs.txt
    #JOB_ID=$(envsubst < /opt/kubernetes/tool.yaml | if [ "%_(JOB_REQUIRES_GPU)s" == "True" ]; then sed '/cerit.io\/gpu.mem/s/^#//'; else cat; fi | kubectl apply -f - | cut -d' ' -f1)

    ### Set environment variable to know running mode is non interactive
    #export XMIPP_IN_QUEUE=1
    # This option is probably not required due to working X11 forwarding
    export XMIPP_IN_QUEUE=0

    K8S_JOB_ID=$(helm template \
        --set instance.namespace=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace) \
        --set instance.name="$INSTANCE_NAME" \
        --set instance.submitpid="$$" \
        --set job.name="%_(JOB_NAME)s" \
        --set job.tool="%_(JOB_TOOL)s" \
        --set job.command="%_(JOB_COMMAND)s" \
        --set job.xmipp_in_queue="$XMIPP_IN_QUEUE" \
        --set job.gpu.required="%_(JOB_REQUIRES_GPU)s" \
        --set job.gpu.dedicated="%_(JOB_GPU_DEDICATED)s" \
        --set job.gpu.shared_mem="%_(JOB_GPU_SHARED_MEM)s" \
        --set od.sid.dataset="$OD_DATASET_SPACEID" \
        --set od.sid.project="$OD_PROJECT_SPACEID" \
        --set instance.mincpu="%_(JOB_THREADS)d" \
        --set instance.maxcpu="%_(JOB_THREADS)d" \
        --set instance.minram="%_(JOB_MEMORY)sGi" \
        --set instance.maxram="%_(JOB_MEMORY)sGi" \
        /opt/kubernetes/chart-tool/ | kubectl apply -f - | cut -d' ' -f1)

    nohup /opt/scipion/job-watchdog.sh </dev/null &>/dev/null "$K8S_JOB_ID" &

    #        ["JOB_TIME", "120", "Time (hours)", "Select the time expected (in hours) for this job"],
    #        ["GPU_COUNT", "1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
    #        ["QUEUE_FOR_JOBS", "N", "Use queue for jobs", "Send individual jobs to queue"]

    echo $$

CANCEL_COMMAND = #!/bin/bash
    export job_id="%_(JOB_ID)s"
    # scipion's var JOB_NAME does not work in the cancel cmd
    export job_name=`kubectl get job | grep "scipion-tool-job-$INSTANCE_NAME" | grep "$job_id" | cut -d"-" -f5`

    kubectl delete job "scipion-tool-job-$INSTANCE_NAME-$job_name-$job_id"

CHECK_COMMAND = touch /home/scipionuser/Desktop/check-%_(JOB_ID)s

QUEUES = {
    "kubernetes-queue": [
        ["JOB_MEMORY", "4", "Memory (GiB)", "Select amount of memory (in gibibytes) for this job."],
        ["JOB_GPU_DEDICATED", "false", "Dedicated GPU (if GPU required)", "Select Dedicated (true) or Shared (false) GPU. Shared GPU will be shared with other users in the cluster. This option is relevant only for GPU-accelerated protocol."],
        ["JOB_GPU_SHARED_MEM", "2", "GPU memory (GiB) (Shared GPU only)", "Select amount of GPU memory (in gibibytes) for this job. This value is relevant only if you choose shared GPU in the previous option."]
    ] }

