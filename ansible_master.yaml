---
- vars:
    vnode_prefix: scipion-wn-
    vnode_master: scipion-master

    docker_runtime: |
      {
        "mtu": 1442
      }
    docker_runtime_nvidia: |
      {
        "mtu": 1442,
        "runtimes": {
          "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
          }
        },
        "default-runtime": "nvidia"
      }

  pre_tasks:
  - name: Create scipionuser group
    group:
      name: scipionuser
      gid: 1042
      state: present

  - name: Create scipionuser user
    user:
      name: scipionuser
      uid: 1042
      shell: /bin/bash
      group: scipionuser

  - name: Add NVIDIA PPA
    apt_repository:
      repo: ppa:graphics-drivers/ppa
      update_cache: yes

  - name: Install nvidia-driver package
    apt:
      name: ['nvidia-driver-440', 'nvidia-cuda-dev']

  - name: Reboot worker
    reboot:
            -
  roles:
  - role: indigo-dc.nfs
    nfs_mode: 'server'
    nfs_exports: [{path: "/home/scipionuser/ScipionUserData", export: "{{ vnode_prefix }}*.localdomain(fsid=0,rw,async,no_root_squash,no_subtree_check,insecure)"}]
  - role: grycap.slurm
    slurm_type_of_node: 'front'
    slurm_server_name: "{{ vnode_master }}"
    slurm_wn_ips: '{{ groups["lrms_wn"]|map("extract", hostvars, "ansible_default_ipv4.address")|list if "lrms_wn" in groups else [] }}'
    slurm_vnode_prefix: "{{ vnode_prefix }}"
    slurm_wn_nodenames: '{{ groups["lrms_wn"]|map("extract", hostvars, "ansible_hostname")|list if "lrms_wn" in groups else [] }}'
    slurm_wn_mem: 4096
    slurm_wn_cpus: 6
    slurm_wn_gres: "gpu:1"
    slurm_wn_gres_tpes: "gpu"
    slurm_wn_gres_conf: "AutoDetect=nvml"
    slurm_user_id: 1052

# Put here your Ansible recipes to configure the nodes

  tasks:

  - name: Install required packages
    apt:
      name: ['xdm', 'curl', 'wget', 'git', 'htop', 'python-pip', 'apt-transport-https', 'ca-certificates', 'gnupg-agent', 'software-properties-common', 'xserver-xorg', 'pkg-config']
      state: present
      update_cache: yes

  - name: Add docker GPG key
    apt_key:
      url: https://download.docker.com/linux/ubuntu/gpg
      state: present
  - name: Get release name
    shell: lsb_release -cs
    register: release_name
  - name: Add docker registry
    apt_repository: repo='deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ release_name.stdout }} stable' state=present

  - name: Install docker
    package:
      name: ['docker-ce', 'docker-ce-cli', 'containerd.io']
      state: present

  - name: Install pip docker
    pip:
      name: docker

  - name: Test GPU support
    command: nvidia-smi
    register: gpu_support
    ignore_errors: yes

  - name: Set mtu value to docker network
    copy:
      content: "{{ docker_runtime }}"
      dest: /etc/docker/daemon.json

  - name: Start docker service
    service:
      name: docker
      state: restarted

  - name: Get release name
    shell: . /etc/os-release;echo $ID$VERSION_ID
    register: distribution

  - name: Add scipionuser to docker group and change shell
    shell: usermod --shell /bin/bash -aG docker scipionuser

  - name: All gpu dependent tasks
    block:
    - name: Add nvidia docker runtime source
      shell: 'curl -s -L https://nvidia.github.io/nvidia-docker/{{ distribution.stdout }}/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list'

    - name: Add nvidia docker runtime GPG 1/2
      apt_key:
        url: https://nvidia.github.io/nvidia-docker/gpgkey
        state: present

    - name: Add nvidia docker runtime GPG 2/2
      apt_key:
        url: https://nvidia.github.io/nvidia-container-runtime/gpgkey
        state: present

    - name: Install nvidia-container-toolkit and nvidia-container-runtime
      apt:
        name: ['nvidia-container-toolkit', 'nvidia-container-runtime']
        update_cache: yes

    - name: Set nvidia as default docker runtime
      copy:
        content: "{{ docker_runtime_nvidia }}"
        dest: /etc/docker/daemon.json

    - name: Restart docker
      service:
        name: docker
        state: restarted

    when: gpu_support is success

  - name: Run Scipion master container
    #shell: docker run -d --name=scipionmaster --hostname=scipion-master --privileged -p 5904:5904 -e USE_DISPLAY="4" -e ROOT_PASS="abc" -e USER_PASS="abc" -e CRYOSPARC_LICENSE="a3dc0cc0-3181-11ea-84d0-8b3771c7f13b" -v /tmp/.X11-unix/X0:/tmp/.X11-unix/X0 -v /home/scipionuser/ScipionUserData:/home/scipionuser/ScipionUserData -v /usr/local/etc/slurm.conf:/usr/local/etc/slurm.conf -v /usr/local/etc/gres.conf:/usr/local/etc/gres.conf -v /etc/munge/munge.key:/etc/munge/munge.key --add-host scipion-master:ip ldelcano/scipion-master:slurm
    docker_container:
      name: scipionmaster
      hostname: "{{ vnode_master }}"
      image: ldelcano/scipion-master:slurm
      privileged: yes
      published_ports:
        - 5904:5904
      env:
        USE_DISPLAY: "4"
        ROOT_PASS: "Scipion4U"
        USER_PASS: "Scipion4U"
        CRYOSPARC_LICENSE: "{{ cryosparc_license }}"
      volumes:
        - /tmp/.X11-unix/X0:/tmp/.X11-unix/X0
        - /home/scipionuser/ScipionUserData:/home/scipionuser/ScipionUserData
        - /usr/local/etc/slurm.conf:/usr/local/etc/slurm.conf
        - /usr/local/etc/gres.conf:/usr/local/etc/gres.conf
        - /etc/munge/munge.key:/etc/munge/munge.key
      etc_hosts:
        {"scipion-master":"{{ slurm_front_end_ip }}"}

